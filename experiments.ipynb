{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 693.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time per attention computation: 0.001457 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, jit\n",
    "from jax.scipy.special import logsumexp\n",
    "from tqdm import tqdm\n",
    "\n",
    "from attention import softmax, scaled_dot_product_attention\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# JIT compile the attention function for faster execution\n",
    "fast_attention = jit(scaled_dot_product_attention)\n",
    "\n",
    "# Generate random data\n",
    "key_len = 512\n",
    "feature_dim = 256\n",
    "rng = random.PRNGKey(0)\n",
    "Q = random.normal(rng, (key_len, feature_dim))\n",
    "K = random.normal(rng, (key_len, feature_dim))\n",
    "V = random.normal(rng, (key_len, feature_dim))\n",
    "\n",
    "# Timing loop\n",
    "import time\n",
    "\n",
    "num_iterations = 1000\n",
    "# Warm up \n",
    "_ = fast_attention(Q, K, V)\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in tqdm(range(num_iterations)):\n",
    "    _ = fast_attention(Q, K, V)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Average time per attention computation: {(end_time - start_time) / num_iterations:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the attention and get the scores\n",
    "_, attention_weights, scores = scaled_dot_product_attention(Q, K, V, return_scores=True)\n",
    "\n",
    "# Plot the attention scores\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(scores, cmap=\"Blues\", aspect='auto')\n",
    "plt.colorbar(label='Attention Scores', orientation='vertical')\n",
    "plt.title('Attention Scores Visualization')\n",
    "plt.xlabel('Keys')\n",
    "plt.ylabel('Queries')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 767.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time per sliding window attention computation: 0.001305 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from attention import sliding_window_attention\n",
    "\n",
    "# JIT compile the sliding window attention function\n",
    "fast_sliding_window_attention = jit(sliding_window_attention)\n",
    "\n",
    "# Warm up\n",
    "_ = fast_sliding_window_attention(Q, K, V, window_size=64)\n",
    "\n",
    "# Timing the sliding window attention\n",
    "start_time = time.time()\n",
    "for _ in tqdm(range(num_iterations)):\n",
    "    _ = fast_sliding_window_attention(Q, K, V, window_size=64)\n",
    "end_time = time.time()\n",
    "print(f\"Average time per sliding window attention computation: {(end_time - start_time) / num_iterations:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the attention and get the scores\n",
    "_, scores = sliding_window_attention(Q, K, V, window_size=64)\n",
    "\n",
    "# Plot the attention scores\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(scores, cmap=\"Blues\", aspect='auto')\n",
    "plt.colorbar(label='Attention Scores', orientation='vertical')\n",
    "plt.title('Attention Scores Visualization')\n",
    "plt.xlabel('Keys')\n",
    "plt.ylabel('Queries')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 126.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time per multi-head vanilla attention: 0.007934 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def split_heads(x, num_heads):\n",
    "    # x.shape = (seq_len, d_model)\n",
    "    # After reshaping: (seq_len, num_heads, depth)\n",
    "    return jnp.reshape(x, (x.shape[0], num_heads, -1))\n",
    "\n",
    "def multi_head_attention(Q, K, V, num_heads, attention_fn, window_size=None):\n",
    "    d_model = Q.shape[-1]\n",
    "    # print(f\"d_model type: {type(d_model)}, value: {d_model}\")\n",
    "    # print(f\"num_heads type: {type(num_heads)}, value: {num_heads}\")\n",
    "    depth = d_model // num_heads\n",
    "    \n",
    "    # Split into multiple heads\n",
    "    Q_heads = split_heads(Q, num_heads)\n",
    "    K_heads = split_heads(K, num_heads)\n",
    "    V_heads = split_heads(V, num_heads)\n",
    "    \n",
    "    # Apply attention to each head\n",
    "    if attention_fn == sliding_window_attention:\n",
    "        assert window_size is not None, \"window_size must be provided for sliding window attention\"\n",
    "        outputs = [attention_fn(Q_heads[:, h], K_heads[:, h], V_heads[:, h], window_size) for h in range(num_heads)]\n",
    "    else:\n",
    "        outputs = [attention_fn(Q_heads[:, h], K_heads[:, h], V_heads[:, h]) for h in range(num_heads)]\n",
    "    \n",
    "    # Concatenate and project\n",
    "    concatenated = jnp.concatenate(outputs, axis=-1)\n",
    "    # Typically, you'd have an additional linear layer here, but for simplicity, we'll skip it\n",
    "    return concatenated\n",
    "\n",
    "# Update JIT compiled functions to support multi-head\n",
    "fast_multi_head_attention = jit(multi_head_attention, static_argnums=(4, 3))\n",
    "\n",
    "# Time multi-head vanilla attention\n",
    "num_heads = 8\n",
    "start_time = time.time()\n",
    "for _ in tqdm(range(num_iterations)):\n",
    "    _ = fast_multi_head_attention(Q=Q, K=K, V=V, num_heads=num_heads, attention_fn=scaled_dot_product_attention)\n",
    "end_time = time.time()\n",
    "print(f\"Average time per multi-head vanilla attention: {(end_time - start_time) / num_iterations:.6f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TracerBoolConversionError",
     "evalue": "Attempted boolean conversion of traced array with shape bool[]..\nThe error occurred while tracing the function grouped_query_attention at /var/folders/yd/npt3q5rj1mvdmlw309mn5d240000gp/T/ipykernel_74694/2317686948.py:6 for jit. This value became a tracer due to JAX operations on these lines:\n\n  operation a:bool[] = eq b c\n    from line /var/folders/yd/npt3q5rj1mvdmlw309mn5d240000gp/T/ipykernel_74694/2317686948.py:7:11 (grouped_query_attention)\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerBoolConversionError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTracerBoolConversionError\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     39\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_iterations):\n\u001b[0;32m---> 40\u001b[0m     _ \u001b[39m=\u001b[39m fast_grouped_query_attention(Q\u001b[39m=\u001b[39;49mQ, K\u001b[39m=\u001b[39;49mK, V\u001b[39m=\u001b[39;49mV, num_heads\u001b[39m=\u001b[39;49mnum_heads, num_groups\u001b[39m=\u001b[39;49mnum_groups, attention_fn\u001b[39m=\u001b[39;49mscaled_dot_product_attention)\n\u001b[1;32m     41\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     42\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAverage time per grouped query vanilla attention: \u001b[39m\u001b[39m{\u001b[39;00m(end_time\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mstart_time)\u001b[39m \u001b[39m\u001b[39m/\u001b[39m\u001b[39m \u001b[39mnum_iterations\u001b[39m:\u001b[39;00m\u001b[39m.6f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m, in \u001b[0;36mgrouped_query_attention\u001b[0;34m(Q, K, V, num_heads, num_groups, attention_fn, window_size)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgrouped_query_attention\u001b[39m(Q, K, V, num_heads, num_groups, attention_fn, window_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m----> 7\u001b[0m     \u001b[39massert\u001b[39;00m num_heads \u001b[39m%\u001b[39m num_groups \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnum_heads should be divisible by num_groups\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m     heads_per_group \u001b[39m=\u001b[39m num_heads \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m num_groups\n\u001b[1;32m     10\u001b[0m     d_model \u001b[39m=\u001b[39m Q\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Projects/AttentionExperiments/.venv/lib/python3.11/site-packages/jax/_src/core.py:1443\u001b[0m, in \u001b[0;36mconcretization_function_error.<locals>.error\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m   1442\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39merror\u001b[39m(\u001b[39mself\u001b[39m, arg):\n\u001b[0;32m-> 1443\u001b[0m   \u001b[39mraise\u001b[39;00m TracerBoolConversionError(arg)\n",
      "\u001b[0;31mTracerBoolConversionError\u001b[0m: Attempted boolean conversion of traced array with shape bool[]..\nThe error occurred while tracing the function grouped_query_attention at /var/folders/yd/npt3q5rj1mvdmlw309mn5d240000gp/T/ipykernel_74694/2317686948.py:6 for jit. This value became a tracer due to JAX operations on these lines:\n\n  operation a:bool[] = eq b c\n    from line /var/folders/yd/npt3q5rj1mvdmlw309mn5d240000gp/T/ipykernel_74694/2317686948.py:7:11 (grouped_query_attention)\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerBoolConversionError"
     ]
    }
   ],
   "source": [
    "def group_heads(x, num_groups):\n",
    "    # x.shape = (seq_len, d_model)\n",
    "    # After reshaping: (seq_len, num_groups, depth * heads_per_group)\n",
    "    return jnp.reshape(x, (x.shape[0], num_groups, -1))\n",
    "\n",
    "def grouped_query_attention(Q, K, V, num_heads, num_groups, attention_fn, window_size=None):\n",
    "    assert num_heads % num_groups == 0, \"num_heads should be divisible by num_groups\"\n",
    "    heads_per_group = num_heads // num_groups\n",
    "\n",
    "    d_model = Q.shape[-1]\n",
    "    depth = d_model // num_heads\n",
    "\n",
    "    # Split keys and values into multiple heads, but queries into groups\n",
    "    Q_groups = group_heads(Q, num_groups)\n",
    "    K_heads = split_heads(K, num_heads)\n",
    "    V_heads = split_heads(V, num_heads)\n",
    "\n",
    "    outputs = []\n",
    "    for g in range(num_groups):\n",
    "        # For each group of queries, compute attention with all key and value heads\n",
    "        Q_group = Q_groups[:, g]\n",
    "        output_heads = [attention_fn(Q_group, K_heads[:, h], V_heads[:, h], window_size) if attention_fn == sliding_window_attention else attention_fn(Q_group, K_heads[:, h], V_heads[:, h]) for h in range(num_heads)]\n",
    "        # Concatenate output heads for this query group\n",
    "        outputs.append(jnp.concatenate(output_heads, axis=-1))\n",
    "    \n",
    "    # Concatenate outputs for all query groups\n",
    "    concatenated = jnp.concatenate(outputs, axis=-1)\n",
    "    return concatenated\n",
    "\n",
    "# JIT compile for speed\n",
    "fast_grouped_query_attention = jit(grouped_query_attention, static_argnums=(5,))\n",
    "\n",
    "# Time grouped query multi-head attention\n",
    "num_groups = 4\n",
    "# Make sure num_heads is divisible by num_groups before calling the JIT-compiled function\n",
    "if num_heads % num_groups != 0:\n",
    "    raise ValueError(\"num_heads should be divisible by num_groups\")\n",
    "start_time = time.time()\n",
    "for _ in range(num_iterations):\n",
    "    _ = fast_grouped_query_attention(Q=Q, K=K, V=V, num_heads=num_heads, num_groups=num_groups, attention_fn=scaled_dot_product_attention)\n",
    "end_time = time.time()\n",
    "print(f\"Average time per grouped query vanilla attention: {(end_time - start_time) / num_iterations:.6f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3 (main, Apr  7 2023, 19:30:05) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86ebd56e26291f5f6381d1a1ffe73e112063ccce3543a1ee4261507f11a766ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
